{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "i5j_dr-gd8jJ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import operator\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# reads a CSV file named iris data andnstores in Pandas DataFrame \n",
        "data = pd.read_csv('iris.csv', header=None, names=['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'class'])\n",
        "print(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AVa59ZvWqAVT",
        "outputId": "6ff0045a-6f2c-4ea9-cc40-7116d7c650ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     sepal_length  sepal_width  petal_length  petal_width      class\n",
            "0             5.1          3.5           1.4          0.2     Setosa\n",
            "1             4.9          3.0           1.4          0.2     Setosa\n",
            "2             4.7          3.2           1.3          0.2     Setosa\n",
            "3             4.6          3.1           1.5          0.2     Setosa\n",
            "4             5.0          3.6           1.4          0.2     Setosa\n",
            "..            ...          ...           ...          ...        ...\n",
            "145           6.7          3.0           5.2          2.3  Virginica\n",
            "146           6.3          2.5           5.0          1.9  Virginica\n",
            "147           6.5          3.0           5.2          2.0  Virginica\n",
            "148           6.2          3.4           5.4          2.3  Virginica\n",
            "149           5.9          3.0           5.1          1.8  Virginica\n",
            "\n",
            "[150 rows x 5 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# generates a randomized array of integers from 0 to the number of rows \n",
        "indices = np.random.permutation(data.shape[0])\n",
        "\n",
        "# calculating the index on which data will be divided \n",
        "div = int(0.75 * len(indices))\n",
        "\n",
        "# dividing the indexes into two array\n",
        "development_id, test_id = indices[:div], indices[div:]\n",
        "\n",
        "# using loc method of pandas dataframe which will select a particular row and its all columns\n",
        "# its just like splitting into training set and testing set\n",
        "\n",
        "# ?????????????????? i think we should exclude last column here ??????????????????? if not then we should in euclidean distance\n",
        "\n",
        "development_set, test_set = data.loc[development_id,:], data.loc[test_id,:]\n",
        "print(\"Development Set:\\n\", development_set, \"\\n\\nTest Set:\\n\", test_set)"
      ],
      "metadata": {
        "id": "ceAO-AwNq1vK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fad587eb-f021-4ef8-9250-1f9cb51f15bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Development Set:\n",
            "      sepal_length  sepal_width  petal_length  petal_width       class\n",
            "37            4.9          3.6           1.4          0.1      Setosa\n",
            "77            6.7          3.0           5.0          1.7  Versicolor\n",
            "119           6.0          2.2           5.0          1.5   Virginica\n",
            "8             4.4          2.9           1.4          0.2      Setosa\n",
            "145           6.7          3.0           5.2          2.3   Virginica\n",
            "..            ...          ...           ...          ...         ...\n",
            "102           7.1          3.0           5.9          2.1   Virginica\n",
            "149           5.9          3.0           5.1          1.8   Virginica\n",
            "76            6.8          2.8           4.8          1.4  Versicolor\n",
            "139           6.9          3.1           5.4          2.1   Virginica\n",
            "143           6.8          3.2           5.9          2.3   Virginica\n",
            "\n",
            "[112 rows x 5 columns] \n",
            "\n",
            "Test Set:\n",
            "      sepal_length  sepal_width  petal_length  petal_width       class\n",
            "4             5.0          3.6           1.4          0.2      Setosa\n",
            "86            6.7          3.1           4.7          1.5  Versicolor\n",
            "22            4.6          3.6           1.0          0.2      Setosa\n",
            "40            5.0          3.5           1.3          0.3      Setosa\n",
            "117           7.7          3.8           6.7          2.2   Virginica\n",
            "118           7.7          2.6           6.9          2.3   Virginica\n",
            "142           5.8          2.7           5.1          1.9   Virginica\n",
            "85            6.0          3.4           4.5          1.6  Versicolor\n",
            "103           6.3          2.9           5.6          1.8   Virginica\n",
            "16            5.4          3.9           1.3          0.4      Setosa\n",
            "112           6.8          3.0           5.5          2.1   Virginica\n",
            "136           6.3          3.4           5.6          2.4   Virginica\n",
            "100           6.3          3.3           6.0          2.5   Virginica\n",
            "93            5.0          2.3           3.3          1.0  Versicolor\n",
            "42            4.4          3.2           1.3          0.2      Setosa\n",
            "95            5.7          3.0           4.2          1.2  Versicolor\n",
            "32            5.2          4.1           1.5          0.1      Setosa\n",
            "66            5.6          3.0           4.5          1.5  Versicolor\n",
            "98            5.1          2.5           3.0          1.1  Versicolor\n",
            "55            5.7          2.8           4.5          1.3  Versicolor\n",
            "0             5.1          3.5           1.4          0.2      Setosa\n",
            "35            5.0          3.2           1.2          0.2      Setosa\n",
            "65            6.7          3.1           4.4          1.4  Versicolor\n",
            "26            5.0          3.4           1.6          0.4      Setosa\n",
            "45            4.8          3.0           1.4          0.3      Setosa\n",
            "29            4.7          3.2           1.6          0.2      Setosa\n",
            "60            5.0          2.0           3.5          1.0  Versicolor\n",
            "11            4.8          3.4           1.6          0.2      Setosa\n",
            "122           7.7          2.8           6.7          2.0   Virginica\n",
            "6             4.6          3.4           1.4          0.3      Setosa\n",
            "106           4.9          2.5           4.5          1.7   Virginica\n",
            "111           6.4          2.7           5.3          1.9   Virginica\n",
            "120           6.9          3.2           5.7          2.3   Virginica\n",
            "113           5.7          2.5           5.0          2.0   Virginica\n",
            "49            5.0          3.3           1.4          0.2      Setosa\n",
            "71            6.1          2.8           4.0          1.3  Versicolor\n",
            "94            5.6          2.7           4.2          1.3  Versicolor\n",
            "104           6.5          3.0           5.8          2.2   Virginica\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# extracting the class labels for development and testing data\n",
        "test_class = list(test_set.iloc[:,-1])\n",
        "dev_class = list(development_set.iloc[:,-1])\n",
        "\n",
        "# calculating mean and standard deviation for both development set and testing set\n",
        "mean_development_set = development_set.mean()\n",
        "mean_test_set = test_set.mean()\n",
        "std_development_set = development_set.std()\n",
        "std_test_set = test_set.std()"
      ],
      "metadata": {
        "id": "d8X1jbuatel1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbda56f1-047e-4cc9-b7eb-f4384fa683f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-30-f2c127012c5f>:6: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
            "  mean_development_set = development_set.mean()\n",
            "<ipython-input-30-f2c127012c5f>:7: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
            "  mean_test_set = test_set.mean()\n",
            "<ipython-input-30-f2c127012c5f>:8: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
            "  std_development_set = development_set.std()\n",
            "<ipython-input-30-f2c127012c5f>:9: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
            "  std_test_set = test_set.std()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# finding Euclidean Distance\n",
        "def euclideanDistance(data_1, data_2, data_len):\n",
        "    dist = 0\n",
        "    for i in range(data_len):\n",
        "        dist = dist + np.square(data_1[i] - data_2[i])\n",
        "    return np.sqrt(dist)\n",
        "\n",
        "# Formula for Normalized Euclidean Distance\n",
        "# d(p, q) = sqrt(sum(((pi - mu_i) / sigma_i - (qi - mu_i) / sigma_i) ** 2))\n",
        "# pi and qi are features of data_1 and data_2 and mu is mean and sigma_i is standard deviation\n",
        "def normalizedEuclideanDistance(data_1, data_2, data_len, data_mean, data_std):\n",
        "    n_dist = 0\n",
        "    for i in range(data_len):\n",
        "        n_dist = n_dist + (np.square(((data_1[i] - data_mean[i])/data_std[i]) - ((data_2[i] - data_mean[i])/data_std[i])))\n",
        "    return np.sqrt(n_dist)\n",
        "\n",
        "\n",
        "def cosineSimilarity(data_1, data_2):\n",
        "# computes the dot product of data_1 and data_2 without considering the last element of data_2.\n",
        "    dot = np.dot(data_1, data_2[:-1])\n",
        "    norm_data_1 = np.linalg.norm(data_1)\n",
        "    norm_data_2 = np.linalg.norm(data_2[:-1])\n",
        "\n",
        "# It computes the cosine similarity between data_1 and data_2, dividing dot by the product of the two Euclidean norms.\n",
        "    cos = dot / (norm_data_1 * norm_data_2)\n",
        "    return (1-cos)\n",
        "\n",
        "\n",
        "# This function calculates the distance between the test instance and all instances\n",
        "# Then it finds the k nearest neighbors and returns the class\n",
        "\n",
        "# For K-nearest neighbours \n",
        "def knn(dataset, testInstance, k, dist_method, dataset_mean, dataset_std): \n",
        "    distances = {}\n",
        "\n",
        "    # ???? why length is depending on test instance ????\n",
        "\n",
        "    length = testInstance.shape[1]\n",
        "    if dist_method == 'euclidean':\n",
        "        for x in range(len(dataset)):\n",
        "            dist_up = euclideanDistance(testInstance, dataset.iloc[x], length)\n",
        "            distances[x] = dist_up[0]\n",
        "    elif dist_method == 'normalized_euclidean':\n",
        "        for x in range(len(dataset)):\n",
        "            dist_up = normalizedEuclideanDistance(testInstance, dataset.iloc[x], length, dataset_mean, dataset_std)\n",
        "            distances[x] = dist_up[0]\n",
        "    elif dist_method == 'cosine':\n",
        "        for x in range(len(dataset)):\n",
        "            dist_up = cosineSimilarity(testInstance, dataset.iloc[x])\n",
        "            distances[x] = dist_up[0]\n",
        "\n",
        "    # Sort values based on distance\n",
        "    sort_distances = sorted(distances.items(), key=operator.itemgetter(1))\n",
        "    neighbors = []\n",
        "    # Extracting nearest k neighbors\n",
        "    for x in range(k):\n",
        "        neighbors.append(sort_distances[x][0])\n",
        "    # Initializing counts for 'class' labels counts as 0\n",
        "    counts = {\"Iris-setosa\" : 0, \"Iris-versicolor\" : 0, \"Iris-virginica\" : 0}\n",
        "    # Computing the most frequent class\n",
        "    for x in range(len(neighbors)):\n",
        "        response = dataset.iloc[neighbors[x]][-1] \n",
        "        if response in counts:\n",
        "            counts[response] += 1\n",
        "        else:\n",
        "            counts[response] = 1\n",
        "    # Sorting the class in reverse order to get the most frequest class\n",
        "    sort_counts = sorted(counts.items(), key=operator.itemgetter(1), reverse=True)\n",
        "    return(sort_counts[0][0])\n",
        "\n",
        "  \n"
      ],
      "metadata": {
        "id": "3EmMkdo2uXlY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now we implememt KNN algorithm on development set for various values of K\n",
        "# It iterates through each data point in the development set \n",
        "# to determine its predicted class label based on the k nearest neighbors\n",
        "\n",
        "row_list = []\n",
        "for index, rows in development_set.iterrows():\n",
        "    my_list =[rows.sepal_length, rows.sepal_width, rows.petal_length, rows.petal_width]       \n",
        "    row_list.append([my_list])\n",
        "# k values for the number of neighbors that need to be considered\n",
        "k_n = [1, 3, 5, 7]\n",
        "# Distance metrics\n",
        "distance_methods = ['euclidean', 'normalized_euclidean', 'cosine']\n",
        "# Performing kNN on the development set by iterating all of the development set data points and for each k and each distance metric\n",
        "obs_k = {}\n",
        "for dist_method in distance_methods:\n",
        "    development_set_obs_k = {}\n",
        "    for k in k_n:\n",
        "        development_set_obs = []\n",
        "        for i in range(len(row_list)):\n",
        "          development_set_obs.append(knn(development_set, pd.DataFrame(row_list[i]), k, dist_method, mean_development_set, std_development_set))\n",
        "        development_set_obs_k[k] = development_set_obs\n",
        "    # Nested Dictionary containing the observed class for each k and each distance metric (obs_k of the form obs_k[dist_method][k])\n",
        "    obs_k[dist_method] = development_set_obs_k\n",
        "    print(dist_method.upper() + \" distance method performed on the dataset for all k values!\")\n",
        "\n"
      ],
      "metadata": {
        "id": "sftWmxPYuXg9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54e78713-8202-495f-b058-952511b68769"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EUCLIDEAN distance method performed on the dataset for all k values!\n",
            "NORMALIZED_EUCLIDEAN distance method performed on the dataset for all k values!\n",
            "COSINE distance method performed on the dataset for all k values!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = {}\n",
        "for key in obs_k.keys():\n",
        "    accuracy[key] = {}\n",
        "    for k_value in obs_k[key].keys():\n",
        "        #print('k = ', key)\n",
        "        count = 0\n",
        "        for i,j in zip(dev_class, obs_k[key][k_value]):\n",
        "            if i == j:\n",
        "                count = count + 1\n",
        "            else:\n",
        "                pass\n",
        "        accuracy[key][k_value] = count/(len(dev_class))\n",
        "\n",
        "# Storing the accuracy for each k and each distance metric into a dataframe\n",
        "df_res = pd.DataFrame({'k': k_n})\n",
        "for key in accuracy.keys():\n",
        "    value = list(accuracy[key].values())\n",
        "    df_res[key] = value\n",
        "print(df_res)\n",
        "\n",
        "# Plotting a Bar Chart for accuracy\n",
        "draw = df_res.plot(x='k', y=['euclidean', 'normalized_euclidean', 'cosine'], kind=\"bar\", colormap='YlGnBu')\n",
        "draw.set(ylabel='Accuracy')\n",
        "\n",
        "# Ignoring k=1 if the value of accuracy for k=1 is 100%, since this mostly implies overfitting\n",
        "df_res.loc[df_res['k'] == 1.0, ['euclidean', 'normalized_euclidean', 'cosine']] = np.nan\n",
        "\n",
        "# Fetching the best k value for using all hyper-parameters\n",
        "# In case the accuracy is the same for different k and different distance metric selecting the first of all the same\n",
        "column_val = [c for c in df_res.columns if not c.startswith('k')]\n",
        "col_max = df_res[column_val].max().idxmax()\n",
        "best_dist_method = col_max\n",
        "row_max = df_res[col_max].argmax()\n",
        "best_k = int(df_res.iloc[row_max]['k'])\n",
        "if df_res.isnull().values.any():\n",
        "    print('\\n\\n\\nBest k value is\\033[1m', best_k, '\\033[0mand best distance metric is\\033[1m', best_dist_method, '\\033[0m. Ignoring k=1 if the value of accuracy for k=1 is 100%, since this mostly implies overfitting')\n",
        "else:\n",
        "    print('\\n\\n\\nBest k value is\\033[1m', best_k, '\\033[0mand best distance metric is\\033[1m', best_dist_method, '\\033[0m.')"
      ],
      "metadata": {
        "id": "PXbueqHU5kTe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "outputId": "7039496c-ba4f-4f83-c910-5ddb3a41d3a5"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   k  euclidean  normalized_euclidean    cosine\n",
            "0  1   1.000000              1.000000  1.000000\n",
            "1  3   0.955357              0.946429  0.982143\n",
            "2  5   0.964286              0.937500  0.964286\n",
            "3  7   0.964286              0.964286  0.964286\n",
            "\n",
            "\n",
            "\n",
            "Best k value is\u001b[1m 3 \u001b[0mand best distance metric is\u001b[1m cosine \u001b[0m. Ignoring k=1 if the value of accuracy for k=1 is 100%, since this mostly implies overfitting\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('\\n\\n\\nBest k value is\\033[1m', best_k, '\\033[0mand best distance metric is\\033[1m', best_dist_method, '\\033[0m')"
      ],
      "metadata": {
        "id": "DNWXmpPf5qEP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98bc5100-6437-4f92-8be9-6e631a799e11"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "Best k value is\u001b[1m 3 \u001b[0mand best distance metric is\u001b[1m cosine \u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a list of list of all columns except 'class' by iterating through the development set\n",
        "row_list_test = []\n",
        "for index, rows in test_set.iterrows(): \n",
        "    my_list =[rows.sepal_length, rows.sepal_width, rows.petal_length, rows.petal_width]       \n",
        "    row_list_test.append([my_list])\n",
        "test_set_obs = []\n",
        "for i in range(len(row_list_test)):\n",
        "    test_set_obs.append(knn(test_set, pd.DataFrame(row_list_test[i]), best_k, best_dist_method, mean_test_set, std_test_set))\n",
        "#print(test_set_obs)\n",
        "\n",
        "count = 0\n",
        "for i,j in zip(test_class, test_set_obs):\n",
        "    if i == j:\n",
        "        count = count + 1\n",
        "    else:\n",
        "        pass\n",
        "accuracy_test = count/(len(test_class))\n",
        "print('Final Accuracy of the Test dataset is ', accuracy_test)"
      ],
      "metadata": {
        "id": "c4NZeDft5tQd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23f1c219-1d7e-4d52-8ec2-416d16b5c51b"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Accuracy of the Test dataset is  1.0\n"
          ]
        }
      ]
    }
  ]
}